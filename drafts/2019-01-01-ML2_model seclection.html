---
layout: post
title:  "机器学习(2):模型选择"
date:   2018-10-07 19:41:00
categories: Computer_Science Machine_Learning
excerpt: "模型选择是整个建模过程中一个十分重要的环节。所谓模型选择，简单来说就是从多个可能模型中选择出一个作为最终的建模结果。如果使用机器学习中的一些术语，我们可以说模型选择是从多种学习算法或拥有不同参数配置的同一学习算法中选择出其中某个学习算法或某种参数配置。笔者本科毕业论文探讨的就是一种重要的模型选择方法（k 折交叉验证）的细节问题，以下内容是笔者论文的开头部分"
permalink: /ML/2/model_selection/
---

<div class="post-style">

<blockquote>参考文献：① 周志华. 机器学习[M]. 北京:清华大学出版社. 2016.01. ② 李航. 统计学习方法[M]. 北京:清华大学出版社. 2012.03.
</blockquote>

<p>模型选择是整个建模过程中一个十分重要的环节。所谓模型选择，简单来说就是从多个可能模型中选择出一个作为最终的建模结果。如果使用机器学习中的一些术语，我们可以说模型选择是从多种学习算法或拥有不同参数配置的同一学习算法中选择出其中某个学习算法或某种参数配置。笔者本科毕业论文探讨的就是一种重要的模型选择方法（k 折交叉验证）的细节问题，以下内容是笔者论文的开头部分：</p>

<blockquote>千百年来，人们可能做过许许多多的美梦，期待着科学的发展能够帮助人们不断逼近世间万物的真相。今天，随着互联网尤其是移动互联网的发展，信息爆炸式增长的时代已然来临，各种各样庞大的数据集正如潮水一般向我们涌来。而统计学，作为一门以数据为中心的学科，正在义不容辞地承担起这个“帮助人们逼近世间万物真相”的重任。在统计学家的世界里，有一件东西很重要，那就是建立模型（简称建模）。建模之于统计学，就像筷子之于中国人的饮食、刀叉之于英国人的饮食一样，不可或缺。统计学家正是用基于数据构建的模型来试图逼近世间万物真相的。</blockquote>

<blockquote>在与 Norman R. Draper 合著的《Empirical Model-Building and Response Surfaces》一书中，英国著名统计学家，Box-Jenkins 模型（即 ARIMA 模型）及 Box-Cox 变换的提出者 George Edward Pelham Box 这样写道：虽然从本质上来说，所有的模型都是错的，但它们中的一些是有用的（原文：essentially, all models are wrong, but some are useful）。这一发人深省的话语告诉人们：统计推断所用模型是基于不完全信息确定的，而信息的不完全使得错误的发生在所难免，然而这并不代表统计建模是毫无意义的骗人把戏，相反，只要我们能将错误控制在一定范围之内，那么这个模型对于实际应用而言就是有用的。</blockquote>

<blockquote>事实上，信息的不完全正是统计推断赖以存在的前提与基础。试想，如果一个人拥有了未卜先知的能力，他能够获得关于未来的所有信息，那么当荧屏上出现诸如天气预报、股价预测等的画面时，他会有怎样的反应呢？笔者觉得，这位天神下凡般的人物可能会在一旁暗自嘲笑道：这帮愚蠢的人类，真是朽木不可雕也。虽然信息的不完全使得统计推断有了它存在的意义与价值，但与此同时，它也使得不同的人甚至同一个人在面对相同问题时，可能建立多种不同的模型。于是，一个现实而重要的问题就摆在了人们的面前，它就是模型的选择。</blockquote>

<blockquote>人们似乎期待着模型选择能够实现这样的功能，即在所有可能模型中，选择出与真实情况最为接近，或换句话说，选择出误差或损失最小的模型。然而遗憾的是，这几乎是不可能的。问题在于：我们无法穷尽所有可能模型，我们并不知道所谓真实情况，我们也没有关于误差或损失的唯一衡量标准。于是，模型选择这样的问题，就像模型建立这样的问题一样，成了一个仁者见仁、智者见智的问题。借用 George Edward Pelham Box 的话说就是：从本质而言，所有的模型选择方法也是错的，但它们中的一些是有用的。</blockquote>

<p class="post-text-noindent">接下来，我们将依次介绍模型选择中的一些基本术语和模型选择的常用方法。</p>

<h1>2.1 基本术语</h1>

<p>我们将模型/学习器的预测输出与样本的真实输出之间的差异称之为<strong>误差</strong> (error)，将模型/学习器在训练集、测试集、新样本上的误差分别称为<strong>训练误差</strong>/<strong>经验误差</strong> (training error / testing error)、<strong>测试误差</strong> (testing error) 和<strong>泛化误差</strong> (generalization error). 对分类问题，我们将分类错误的样本数占样本总数的比例称之为<strong>错误率</strong> (error rate)，将“1-错误率”称之为<strong>精度</strong> (accuracy).</p>

<p>对给定的某个数据集，当模型/学习器将训练样本自身的一些特点当作所有潜在样本都具有的一般性质时，我们将称该模型/学习器<strong>过拟合</strong> (overfitting) 或过配。如果学习器对训练样本的一般性质尚未学好，我们称该模型/学习器<strong>欠拟合</strong> (underfitting) 或欠配。</p>

<blockquote>过拟合是一个无法彻底避免的现象，因为机器学习面临的问题是 NP 难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成的，若可彻底避免过拟合，则通过经验误差最小化就能获得最优解，而这意味着我们构造性地证明了 P=NP，这种自相矛盾告诉我们过拟合无法彻底避免，因此机器学习的各类算法都必然带有一些针对过拟合的措施。欠拟合则比较容易克服，例如在决策树学习中扩展分支，在神经网络学习中增加训练轮数等。</blockquote>

<blockquote>关于 NP 难及其所涉及的计算理论，笔者将在学习完计算机科学导论中有关计算理论的部分后于专题文章中加以阐述，这里如果读者不是很懂这些概念也没关系，暂时跳过吧。</blockquote>

<p>我们将对算法参数进行设定的过程称之为<strong>调参</strong> (parameter tuning)，在很多强大的学习算法中有不少参数需要设定，这将导致极大的调参工程量，以至于在不少应用任务中，参数调得好不好往往对最终模型性能有关键性影响。此外，有一点值得注意，那就是：学习算法的很多参数在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的。现实中常用的做法是对每个参数选定一个范围和变化步长。</p>

<h1>2.2 常用方法</h1>

<p>本节，笔者将试着从一个统一的框架出发，对有监督学习中常见的模型选择方法做一个系统性的阐述。总的来说，我们期待选择这样一个模型，它对未知数据的预测效果最好。然而，我们没有办法准确计算一个模型对未知数据的预测效果，我们只能通过某些方法去估计一个模型对未知数据的预测效果，与此同时，究竟什么叫预测效果好，并没有一个统一的衡量标准，它取决于实际问题、实际需求、看待问题的角度等等。于是，对模型实际预测效果进行估计的不同方法与衡量模型预测效果的不同标准的组合，构成了多种多样的模型选择方法。</p>

<p>笔者将对模型实际预测效果进行估计的方法概括为如下三大类：</p>

<ul>
	<li>利用模型在样本内的预测效果估计模型实际预测效果：如样本内预测均方误差、样本内分类错误率等；</li>
	<li>利用模型在样本内的预测效果与模型复杂度的加权和估计模型实际预测效果：如 AIC、BIC 等；</li>
	<li>利用模型在样本外的预测效果估计模型实际预测效果：如交叉验证、包外估计等。</li>
</ul>

<p class="post-text-noindent">将衡量模型预测效果的标准概括为：</p>

<ul>
	<li>经验风险（empirical risk）：损失函数的平均</li>
	$$R_{emp} = \frac{1}{n} \sum_{i=1}^n L \left(y_i,f \left(\boldsymbol{x}_i \right ) \right )$$
	<li>查准率、查全率、$F_\beta$ 度量</li>
	$$P = \frac{TP}{TP + FP}$$
	$$R = \frac{TP}{TP + FN}$$
	$$F_\beta = \frac{\left(1+\beta^2 \right )PR}{\left(\beta^2P \right )+R}$$
	<li>TPR、FPR、Youden</li>
	$$TPR = \frac{TP}{TP + FN}$$
	$$FPR = \frac{FP}{TN + FP}$$
	$$Youden = TPR - FPR$$
	<li>代价敏感错误率</li>
	$$$$
</ul>

</div>